{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaysulk/AFNO-transformer/blob/master/Experiment_1_Hartlrey_vs_Fourier_Neural_Operator_on_2D_Burger's_PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5mCFjh_BMdB"
      },
      "source": [
        "#Experiment 1: Hartlrey Neural Operator on 2D Burger's PDE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UONmBaRWRyZ5"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dug0YnfVCcTV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "import math\n",
        "import pickle\n",
        "from datetime import timedelta\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.filters import gaussian\n",
        "from tqdm import tqdm\n",
        "import psutil  # For CPU memory tracking\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor, vmap\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "from typing import List, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_W9lPrLR3jD"
      },
      "source": [
        "###Set CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE9Ti5P-NpdM"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_6MDKheed8y",
        "outputId": "5107e1ae-6af8-4438-c47e-8c0fc769c7c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv4sAp1bR9zc"
      },
      "source": [
        "###Mount Google Drive for data and figure saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqZIIlATBctc",
        "outputId": "134a6e94-c450-40a7-aa99-7337cf36bba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yg2J1sfSGBC"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cqJqBkIB3gM"
      },
      "outputs": [],
      "source": [
        "class SpectralConv3d(nn.Module):\n",
        "    \"\"\"\n",
        "    A spectral convolution layer that applies the Discrete Hartley Transform (DHT)\n",
        "    to perform convolution in the frequency domain for 3D tensors.\n",
        "\n",
        "    This layer takes 3D input tensors and applies a convolution operation using a\n",
        "    specified number of Hartley modes, incorporating a phase factor W[k].\n",
        "\n",
        "    Attributes:\n",
        "        in_channels (int): Number of input channels.\n",
        "        out_channels (int): Number of output channels.\n",
        "        modes1 (int): Number of Hartley modes to use in the first dimension.\n",
        "        modes2 (int): Number of Hartley modes to use in the second dimension.\n",
        "        modes3 (int): Number of Hartley modes to use in the third dimension.\n",
        "        scale (float): Scaling factor for the weights initialization.\n",
        "        weights1 (nn.Parameter): Learnable weights for the convolution, initialized randomly.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, modes1, modes2, modes3):\n",
        "        \"\"\"\n",
        "        Initialize the SpectralConv3d layer.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels.\n",
        "            out_channels (int): Number of output channels.\n",
        "            modes1 (int): Number of Hartley modes in the first dimension.\n",
        "            modes2 (int): Number of Hartley modes in the second dimension.\n",
        "            modes3 (int): Number of Hartley modes in the third dimension.\n",
        "        \"\"\"\n",
        "        super(SpectralConv3d, self).__init__()  # Call to the parent class constructor\n",
        "        self.in_channels = in_channels  # Store the number of input channels\n",
        "        self.out_channels = out_channels  # Store the number of output channels\n",
        "        self.modes1 = modes1  # Store the number of modes for the first dimension\n",
        "        self.modes2 = modes2  # Store the number of modes for the second dimension\n",
        "        self.modes3 = modes3  # Store the number of modes for the third dimension\n",
        "\n",
        "        self.scale = (1 / (in_channels * out_channels))  # Compute scaling factor\n",
        "        self.weights1 = nn.Parameter(  # Initialize learnable weights\n",
        "            self.scale * torch.rand(\n",
        "                in_channels, out_channels, self.modes1, self.modes2, self.modes3\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def dht_3d(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform the Discrete Hartley Transform (DHT) on a 3D tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of arbitrary shape. The last three dimensions\n",
        "                              are assumed to be the dimensions to transform.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Transformed tensor after applying the DHT.\n",
        "        \"\"\"\n",
        "        # Transform over the last three dimensions\n",
        "        transform_dims = [-3, -2, -1]\n",
        "        # Compute the FFT without normalization\n",
        "        X = torch.fft.fftn(x, dim=transform_dims)\n",
        "        # Compute the DHT by combining real and imaginary parts\n",
        "        X = X.real - X.imag\n",
        "        return X\n",
        "\n",
        "    def idht_3d(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform the Inverse Discrete Hartley Transform (IDHT) on a 3D tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of arbitrary shape. The last three dimensions\n",
        "                              are assumed to be the dimensions to transform.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Inverse transformed tensor after applying the IDHT.\n",
        "        \"\"\"\n",
        "        # The inverse DHT is the same as the DHT\n",
        "        X = self.dht_3d(x)\n",
        "        # Multiply by the scaling factor\n",
        "        scaling_factor = x.shape[-3] * x.shape[-2] * x.shape[-1]\n",
        "        X = X / scaling_factor\n",
        "        return X\n",
        "\n",
        "    def compl_mul3d(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform Hartley convolution by flipping, rolling, and combining in the Hartley domain.\n",
        "\n",
        "        This method follows the convolution approach:\n",
        "            Z = 0.5 * (X * (Y + Y_flip) + X_flip * (Y - Y_flip))\n",
        "\n",
        "        Args:\n",
        "            x1 (torch.Tensor): Hartley transformed input tensor of shape (batch_size, in_channels, D, H, W).\n",
        "            x2 (torch.Tensor): Hartley transformed weights tensor of shape (in_channels, out_channels, D, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Hartley transformed output tensor of shape (batch_size, out_channels, D, H, W).\n",
        "        \"\"\"\n",
        "        # Flip and roll x1 and x2 along the spatial dimensions\n",
        "        # Flip along depth, height, and width (last three dimensions)\n",
        "        x1_flip = torch.roll(torch.flip(x1, dims=[-3, -2, -1]), shifts=(1, 1, 1), dims=[-3, -2, -1])\n",
        "        x2_flip = torch.roll(torch.flip(x2, dims=[-3, -2, -1]), shifts=(1, 1, 1), dims=[-3, -2, -1])\n",
        "\n",
        "        # Compute Y_plus and Y_minus\n",
        "        Y_plus = x2 + x2_flip  # Shape: (in_channels, out_channels, D, H, W)\n",
        "        Y_minus = x2 - x2_flip  # Shape: (in_channels, out_channels, D, H, W)\n",
        "\n",
        "        # Perform element-wise multiplication and sum over in_channels\n",
        "        # Expand dimensions for broadcasting\n",
        "        # x1: (batch_size, in_channels, D, H, W) -> (batch_size, in_channels, 1, D, H, W)\n",
        "        # Y_plus, Y_minus: (in_channels, out_channels, D, H, W) -> (1, in_channels, out_channels, D, H, W)\n",
        "        X = x1.unsqueeze(2)  # (batch_size, in_channels, 1, D, H, W)\n",
        "        X_flip = x1_flip.unsqueeze(2)  # (batch_size, in_channels, 1, D, H, W)\n",
        "\n",
        "        # Compute the products\n",
        "        term1 = X * Y_plus  # (batch_size, in_channels, out_channels, D, H, W)\n",
        "        term2 = X_flip * Y_minus  # (batch_size, in_channels, out_channels, D, H, W)\n",
        "\n",
        "        # Combine the terms\n",
        "        Z = 0.5 * (term1 + term2)  # (batch_size, in_channels, out_channels, D, H, W)\n",
        "\n",
        "        # Sum over the input channels to get the final output\n",
        "        Z = Z.sum(dim=1)  # (batch_size, out_channels, D, H, W)\n",
        "\n",
        "        return Z\n",
        "\n",
        "\n",
        "#    def compl_mul3d(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform the complex multiplication of two 3D tensors in the frequency domain.\n",
        "\n",
        "        This function computes the product of two complex-valued tensors using a specific\n",
        "        method that involves flipping the tensors and performing Einstein summation.\n",
        "\n",
        "        Args:\n",
        "            x1 (torch.Tensor): The first input tensor of shape (batch_size, in_channels, depth, height, width).\n",
        "            x2 (torch.Tensor): The second input tensor of the same shape as x1.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The result of the complex multiplication, with shape\n",
        "                          (batch_size, out_channels, depth, height, width).\n",
        "        \"\"\"\n",
        "#        X1_H_k = x1  # Forward tensor for x1\n",
        "#        X2_H_k = x2  # Forward tensor for x2\n",
        "#        X1_H_neg_k = self.flip_periodic_3d(x1)  # Flipped tensor for x1\n",
        "#        X2_H_neg_k = self.flip_periodic_3d(x2)  # Flipped tensor for x2\n",
        "\n",
        "        # Calculate the result using Einstein summation for complex multiplication\n",
        "#        result = 0.5 * (\n",
        "#            torch.einsum('bixyz,ioxyz->boxyz', X1_H_k, X2_H_k) -  # Term 1: Forward multiplication\n",
        "#            torch.einsum('bixyz,ioxyz->boxyz', X1_H_neg_k, X2_H_neg_k) +  # Term 2: Flipped multiplication\n",
        "#            torch.einsum('bixyz,ioxyz->boxyz', X1_H_k, X2_H_neg_k) +  # Term 3: Mixed multiplication\n",
        "#            torch.einsum('bixyz,ioxyz->boxyz', X1_H_neg_k, X2_H_k)  # Term 4: Mixed multiplication\n",
        "#        )\n",
        "\n",
        "#        return result  # Return the result of the complex multiplication\n",
        "\n",
        "\n",
        "\n",
        "    def flip_periodic_3d(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Flip the input tensor along all three dimensions, maintaining periodicity.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, channels, depth, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Flipped tensor of the same shape.\n",
        "        \"\"\"\n",
        "        # Flipping operation to maintain periodicity\n",
        "        return torch.flip(x, dims=[2, 3, 4])  # Flip along depth, height, and width\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the spectral convolution layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth, height, width).\n",
        "\n",
        "        The forward pass consists of the following steps:\n",
        "            1. Compute the batch size and input tensor sizes.\n",
        "            2. Apply the Discrete Hartley Transform (DHT) to the input tensor to obtain the Hartley coefficients.\n",
        "            3. Compute the phase factor W[k] for each relevant frequency mode.\n",
        "            4. Apply the phase factor to the Hartley coefficients.\n",
        "            5. Initialize the output tensor for the transformed coefficients.\n",
        "            6. Perform complex multiplication of the relevant Hartley modes using the learned weights.\n",
        "            7. Apply the Inverse DHT (IDHT) to transform the output back to physical space.\n",
        "        \"\"\"\n",
        "        batchsize = x.shape[0]  # Get the batch size\n",
        "        size1, size2, size3 = x.shape[-3], x.shape[-2], x.shape[-1]  # Get the sizes of the input tensor\n",
        "\n",
        "        # Compute Hartley coefficients\n",
        "        x_ht = self.dht_3d(x)  # Apply the DHT to obtain Hartley coefficients\n",
        "\n",
        "        # Compute the phase factor W[k] for each mode in all three dimensions\n",
        "        # W[k] = cos(2 * pi * k / N) + sin(2 * pi * k / N)\n",
        "        # where k is the mode index and N is the size in that dimension\n",
        "\n",
        "        # Compute mode indices\n",
        "        k1 = torch.arange(self.modes1, device=x.device).unsqueeze(1).unsqueeze(2)  # Shape: (modes1, 1, 1)\n",
        "        k2 = torch.arange(self.modes2, device=x.device).unsqueeze(0).unsqueeze(2)  # Shape: (1, modes2, 1)\n",
        "        k3 = torch.arange(self.modes3, device=x.device).unsqueeze(0).unsqueeze(1)  # Shape: (1, 1, modes3)\n",
        "\n",
        "        # Compute theta for each dimension\n",
        "        theta1 = 2 * np.pi * k1 / size1  # Shape: (modes1, 1, 1)\n",
        "        theta2 = 2 * np.pi * k2 / size2  # Shape: (1, modes2, 1)\n",
        "        theta3 = 2 * np.pi * k3 / size3  # Shape: (1, 1, modes3)\n",
        "\n",
        "        # Compute W for each dimension\n",
        "        W1 = torch.cos(theta1) + torch.sin(theta1)  # Shape: (modes1, 1, 1)\n",
        "        W2 = torch.cos(theta2) + torch.sin(theta2)  # Shape: (1, modes2, 1)\n",
        "        W3 = torch.cos(theta3) + torch.sin(theta3)  # Shape: (1, 1, modes3)\n",
        "\n",
        "        # Combine W for all three dimensions using broadcasting\n",
        "        W = W1 * W2 * W3  # Shape: (modes1, modes2, modes3)\n",
        "\n",
        "        # Reshape W to match the Hartley coefficients tensor shape\n",
        "        W = W.view(1, 1, self.modes1, self.modes2, self.modes3)  # Shape: (1, 1, modes1, modes2, modes3)\n",
        "\n",
        "        # Apply the phase factor to the Hartley coefficients\n",
        "        x_ht = x_ht[:, :, :self.modes1, :self.modes2, :self.modes3] * W  # Element-wise multiplication\n",
        "\n",
        "        # Initialize the output tensor for the transformed coefficients\n",
        "        out_ht = torch.zeros(\n",
        "            batchsize,\n",
        "            self.out_channels,\n",
        "            size1,\n",
        "            size2,\n",
        "            size3,\n",
        "            device=x.device,  # Ensure the output tensor is on the same device as the input\n",
        "            dtype=x.dtype  # Ensure the output tensor has the same data type as the input\n",
        "        )\n",
        "\n",
        "        # Perform complex multiplication on the relevant Hartley modes\n",
        "        out_ht[:, :, :self.modes1, :self.modes2, :self.modes3] = self.compl_mul3d(\n",
        "            x_ht,  # Hartley coefficients after applying phase factor\n",
        "            self.weights1  # Learnable weights\n",
        "        )\n",
        "\n",
        "        # Return to physical space using the Inverse DHT\n",
        "        x = self.idht_3d(out_ht)  # Apply the IDHT to get the output in physical space\n",
        "\n",
        "        return x  # Return the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzQjU3DgEv5R"
      },
      "outputs": [],
      "source": [
        "class FNN3d(nn.Module):\n",
        "    \"\"\"\n",
        "    A 3D Feedforward Neural Network (FNN) that utilizes spectral convolutions\n",
        "    and fully connected layers to process 3D input data.\n",
        "\n",
        "    Attributes:\n",
        "        modes1 (list of int): List of maximal modes for the first dimension for each layer.\n",
        "        modes2 (list of int): List of maximal modes for the second dimension for each layer.\n",
        "        modes3 (list of int): List of maximal modes for the third dimension for each layer.\n",
        "        in_dim (int): Input dimension.\n",
        "        out_dim (int): Output dimension.\n",
        "        padding (tuple): Padding values for each dimension.\n",
        "        layers (list of int): List defining the number of channels in each layer.\n",
        "        fc0 (nn.Linear): Fully connected layer for initial input processing.\n",
        "        sp_convs (nn.ModuleList): List of spectral convolution layers.\n",
        "        ws (nn.ModuleList): List of 1D convolution layers for processing the output of spectral convolutions.\n",
        "        fc1 (nn.Linear): Fully connected layer for intermediate output processing.\n",
        "        fc2 (nn.Linear): Final fully connected layer to produce output.\n",
        "        activations (nn.ModuleList or callable): Activation functions to be used in the network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, modes1, modes2, modes3, width=16, fc_dim=128, layers=None, in_dim=4, out_dim=1, activation='tanh', pad_x=0, pad_y=0, pad_z=0):\n",
        "        \"\"\"\n",
        "        Initialize the FNN3d network.\n",
        "\n",
        "        Args:\n",
        "            modes1 (list of int): First dimension maximal modes for each layer.\n",
        "            modes2 (list of int): Second dimension maximal modes for each layer.\n",
        "            modes3 (list of int): Third dimension maximal modes for each layer.\n",
        "            width (int): Width of the layers if layers is not provided.\n",
        "            fc_dim (int): Dimension of the fully connected layers.\n",
        "            layers (list of int): List of integers defining channels for each layer.\n",
        "            in_dim (int): Input dimension.\n",
        "            out_dim (int): Output dimension.\n",
        "            activation (str): Activation function to be used ('tanh', 'gelu', 'relu', 'elu', 'swish', 'leaky_relu', 'prelu', 'sine', 'sinc').\n",
        "            pad_x (int): Padding for the x dimension.\n",
        "            pad_y (int): Padding for the y dimension.\n",
        "            pad_z (int): Padding for the z dimension.\n",
        "        \"\"\"\n",
        "        super(FNN3d, self).__init__()  # Call to the parent class constructor\n",
        "        self.modes1 = modes1  # Store the modes for the first dimension\n",
        "        self.modes2 = modes2  # Store the modes for the second dimension\n",
        "        self.modes3 = modes3  # Store the modes for the third dimension\n",
        "        self.in_dim = in_dim  # Store input dimension\n",
        "        self.out_dim = out_dim  # Store output dimension\n",
        "        self.padding = (0, 0, 0, pad_z, 0, pad_y, 0, pad_x)  # Define padding for each dimension\n",
        "\n",
        "        # Initialize layers\n",
        "        if layers is None:\n",
        "            self.layers = [width] * 3  # Default to a list of three layers if none provided\n",
        "        else:\n",
        "            self.layers = layers  # Use provided layers\n",
        "        self.fc0 = nn.Linear(self.in_dim, self.layers[0])  # First fully connected layer\n",
        "\n",
        "        # Create spectral convolution layers\n",
        "        self.sp_convs = nn.ModuleList([SpectralConv3d(\n",
        "            in_size, out_size, mode1_num, mode2_num, mode3_num)\n",
        "            for in_size, out_size, mode1_num, mode2_num, mode3_num\n",
        "            in zip(self.layers, self.layers[1:], self.modes1, self.modes2, self.modes3)])  # List of spectral convolution layers\n",
        "\n",
        "        # Create 1D convolution layers\n",
        "        self.ws = nn.ModuleList([nn.Conv1d(in_size, out_size, 1)\n",
        "                                 for in_size, out_size in zip(self.layers, self.layers[1:])])  # List of 1D convolutions for residual connections\n",
        "\n",
        "        self.fc1 = nn.Linear(self.layers[-1], fc_dim)  # Fully connected layer for intermediate processing\n",
        "        self.fc2 = nn.Linear(fc_dim, self.out_dim)  # Final fully connected layer to produce output\n",
        "\n",
        "        # Initialize activations\n",
        "        if activation == 'tanh':\n",
        "            self.activation = F.tanh  # Tanh activation\n",
        "        elif activation == 'gelu':\n",
        "            self.activation = F.gelu  # GELU activation\n",
        "        elif activation == 'relu':\n",
        "            self.activation = F.relu  # ReLU activation\n",
        "        elif activation == 'elu':\n",
        "            self.activation = F.elu  # ELU activation\n",
        "        elif activation == 'swish':\n",
        "            self.activation = self.swish  # Swish activation\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.activation = F.leaky_relu  # Leaky ReLU activation\n",
        "        elif activation == 'prelu':\n",
        "            self.activation = nn.PReLU()  # PReLU activation\n",
        "        elif activation == 'sine':\n",
        "            # Create a ModuleList of SineActivationLearnable for each layer\n",
        "            self.activations = nn.ModuleList([SineActivationLearnable(num_features=layer) for layer in self.layers[1:]])\n",
        "        elif activation == 'sinc':\n",
        "            self.activation = self.sinc  # Sinc activation\n",
        "        else:\n",
        "            raise ValueError(f'{activation} is not supported')  # Raise error for unsupported activation functions\n",
        "\n",
        "    def swish(self, x):\n",
        "        \"\"\"Swish activation function: x * sigmoid(x)\"\"\"\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "    def sinc(self, x):\n",
        "        \"\"\"Sinc activation function: sinc(x) = sin(x) / x (with sinc(0) = 1)\"\"\"\n",
        "        return torch.where(x != 0, torch.sin(x) / x, torch.ones_like(x))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the FNN3d network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batchsize, x_grid, y_grid, z_grid, in_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batchsize, x_grid, y_grid, z_grid, out_dim).\n",
        "\n",
        "        The forward pass consists of the following steps:\n",
        "            1. Determine the batch size and dimensions of the input tensor.\n",
        "            2. Apply padding to the input tensor.\n",
        "            3. Process the input through the first fully connected layer.\n",
        "            4. Permute the dimensions of the tensor for compatibility with spectral convolutions.\n",
        "            5. Sequentially apply spectral convolutions and 1D convolutions,\n",
        "               with activation functions in between.\n",
        "            6. Permute the output back to original dimensions.\n",
        "            7. Pass through fully connected layers to produce final output.\n",
        "            8. Reshape the output tensor to ensure dimensions are as expected.\n",
        "        \"\"\"\n",
        "        length = len(self.ws)  # Get the number of 1D convolution layers\n",
        "        batchsize = x.shape[0]  # Get the batch size\n",
        "        nx, ny, nz = x.shape[1], x.shape[2], x.shape[3]  # Get the dimensions of the input tensor\n",
        "        x = F.pad(x, self.padding, \"constant\", 0)  # Pad the input tensor\n",
        "        size_x, size_y, size_z = x.shape[1], x.shape[2], x.shape[3]  # Get the new dimensions after padding\n",
        "\n",
        "        x = self.fc0(x)  # Process through the first fully connected layer\n",
        "        x = x.permute(0, 4, 1, 2, 3)  # Permute dimensions for spectral convolutions\n",
        "\n",
        "        # Sequentially apply spectral and 1D convolutions\n",
        "        for i, (speconv, w) in enumerate(zip(self.sp_convs, self.ws)):\n",
        "            x1 = speconv(x)  # Apply spectral convolution\n",
        "            x2 = w(x.view(batchsize, self.layers[i], -1)).view(batchsize, self.layers[i+1], size_x, size_y, size_z)  # Apply 1D convolution\n",
        "            x = x1 + x2  # Combine the results\n",
        "            if i != length - 1:\n",
        "                if hasattr(self, 'activations'):\n",
        "                    x = self.activations[i](x)  # Apply SineActivationLearnable\n",
        "                else:\n",
        "                    x = self.activation(x)  # Apply standard activation function\n",
        "\n",
        "        x = x.permute(0, 2, 3, 4, 1)  # Permute back to original dimensions\n",
        "        x = self.fc1(x)  # Process through the first fully connected layer\n",
        "        if hasattr(self, 'activations'):\n",
        "            # If activations are defined as ModuleList (e.g., 'sine'), apply activations to fc1 output\n",
        "            # Assuming fc1 output has shape (batchsize, x_grid, y_grid, z_grid, fc_dim)\n",
        "            # We'll apply activation across the last dimension (fc_dim)\n",
        "            # Reshape to (batchsize * x_grid * y_grid * z_grid, fc_dim) for activation\n",
        "            x = x.view(-1, x.shape[-1])\n",
        "            x = self.activations[-1](x)\n",
        "            x = x.view(batchsize, size_x, size_y, size_z, -1)\n",
        "        else:\n",
        "            x = self.activation(x)  # Apply activation function\n",
        "        x = self.fc2(x)  # Process through the final fully connected layer\n",
        "        x = x.reshape(batchsize, size_x, size_y, size_z, self.out_dim)  # Ensure output dimensions are correct\n",
        "        x = x[..., :nx, :ny, :nz]  # Remove any padding before returning the output\n",
        "        return x  # Return the final output tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWOll295GUHw"
      },
      "outputs": [],
      "source": [
        "def FDM_Burgers2D(u, D=1, nu=0.01):\n",
        "    \"\"\"\n",
        "    Compute the right-hand side of the 2D Burgers' equation using finite difference methods.\n",
        "\n",
        "    This function calculates the spatial and temporal derivatives of the input tensor\n",
        "    representing the velocity field in a 2D Burgers' equation, including the effects\n",
        "    of nonlinearity and diffusion.\n",
        "\n",
        "    Args:\n",
        "        u (torch.Tensor): Input tensor of shape (batchsize, nx, ny, nt) representing the velocity field.\n",
        "        D (float): Domain size (length) of the spatial grid.\n",
        "        nu (float): Viscosity coefficient for the diffusion term.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Derivative tensor representing the changes in the velocity field.\n",
        "    \"\"\"\n",
        "    batchsize = u.size(0)  # Number of samples in the batch\n",
        "    nx = u.size(1)  # Number of grid points in the x direction\n",
        "    ny = u.size(2)  # Number of grid points in the y direction\n",
        "    nt = u.size(3)  # Number of time steps\n",
        "    u = u.reshape(batchsize, nx, ny, nt)  # Reshape u to 4D tensor\n",
        "\n",
        "    dt = D / (nt - 1)  # Time step size\n",
        "    dx = D / nx  # Grid spacing in the x direction\n",
        "    dy = D / ny  # Grid spacing in the y direction\n",
        "\n",
        "    # Calculate spatial derivatives using torch.gradient\n",
        "    u_x, u_y = torch.gradient(u, dim=[1, 2])  # First derivatives in x and y directions\n",
        "\n",
        "    # Calculate second derivatives (Laplacian)\n",
        "    u_xx, _ = torch.gradient(u_x, dim=[1, 2])  # Second derivative in x\n",
        "    _, u_yy = torch.gradient(u_y, dim=[1, 2])  # Second derivative in y\n",
        "\n",
        "    # Time derivative (central difference)\n",
        "    ut = (u[..., 2:] - u[..., :-2]) / (2 * dt)  # Central difference in time\n",
        "\n",
        "    # Compute the nonlinear term and diffusion\n",
        "    Du = ut + (0.5 * (u_x ** 2 + u_y ** 2) - nu * (u_xx + u_yy))[..., 1:-1]  # Combine results\n",
        "\n",
        "    return Du  # Return the computed derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfYJwcM7GYmk"
      },
      "outputs": [],
      "source": [
        "def PINO_loss_burgers2D(u, u0, nu=0.01):\n",
        "    \"\"\"\n",
        "    Calculate the loss for the PINO model applied to the 2D Burgers' equation.\n",
        "\n",
        "    This function computes the initial condition loss and the loss associated with\n",
        "    the right-hand side of the Burgers' equation.\n",
        "\n",
        "    Args:\n",
        "        u (torch.Tensor): Predicted tensor of shape (batchsize, nx, ny, nt) representing the velocity field.\n",
        "        u0 (torch.Tensor): Initial condition tensor of shape (batchsize, nx, ny) to compare against.\n",
        "        nu (float): Viscosity coefficient for the diffusion term.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the initial condition loss and the loss for the right-hand side of the equation.\n",
        "    \"\"\"\n",
        "    batchsize = u.size(0)  # Number of samples in the batch\n",
        "    nx = u.size(1)  # Number of grid points in the x direction\n",
        "    ny = u.size(2)  # Number of grid points in the y direction\n",
        "    nt = u.size(3)  # Number of time steps\n",
        "    u = u.reshape(batchsize, nx, ny, nt)  # Reshape u to 4D tensor\n",
        "\n",
        "    lploss = F.mse_loss  # Use Mean Squared Error loss for initial condition\n",
        "    u_ic = u[..., 0]  # Extract initial condition from predicted velocity field\n",
        "    loss_ic = lploss(u_ic, u0)  # Compute loss against the true initial condition\n",
        "\n",
        "    # Compute the derivatives using the finite difference method\n",
        "    Du = FDM_Burgers2D(u, nu=nu)  # Get the derivative tensor for the current velocity field\n",
        "    f = torch.zeros(Du.shape, device=u.device)  # Target tensor for the PDE right-hand side\n",
        "    loss_f = F.mse_loss(Du, f)  # Compute the loss for the right-hand side of the equation\n",
        "\n",
        "    return loss_ic, loss_f  # Return both losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFJ5r-fDHf1S"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains classes for solving the 2D Burgers' equation using finite difference methods.\n",
        "The Burgers' equation is a fundamental partial differential equation that describes various physical phenomena,\n",
        "including fluid dynamics and shock wave formation.\n",
        "\n",
        "The module includes two classes:\n",
        "\n",
        "1. BurgersEq2D: This class implements the 2D Burgers' equation using scalar velocity.\n",
        "   - It initializes the spatial domain, grid points, and physical parameters such as viscosity.\n",
        "   - It provides methods for computing spatial derivatives using central differencing,\n",
        "   calculating the right-hand side (RHS) of the equation, and performing time-stepping via\n",
        "   the Runge-Kutta 4th order (RK4) method.\n",
        "   - The class includes a method for plotting the velocity field at each time step.\n",
        "\n",
        "   Inputs:\n",
        "   - xmin: Minimum x-coordinate of the spatial domain (default 0)\n",
        "   - xmax: Maximum x-coordinate of the spatial domain (default 1)\n",
        "   - ymin: Minimum y-coordinate of the spatial domain (default 0)\n",
        "   - ymax: Maximum y-coordinate of the spatial domain (default 1)\n",
        "   - Nx: Number of grid points in the x-direction (default 100)\n",
        "   - Ny: Number of grid points in the y-direction (default 100)\n",
        "   - nu: Viscosity coefficient (default 0.01)\n",
        "   - dt: Time step (default 1e-3)\n",
        "   - tend: End time for the simulation (default 1.0)\n",
        "   - device: Device (CPU/GPU) to use (default None)\n",
        "   - dtype: Data type for tensors (default torch.float64)\n",
        "\n",
        "   Outputs:\n",
        "   - A tensor containing the time evolution of the velocity field.\n",
        "\n",
        "2. BurgersEq2D_Vec: This class extends the BurgersEq2D class to handle vector fields,\n",
        "   allowing for the simulation of two velocity components (u and v) in the 2D space.\n",
        "   - It includes similar methods for computing derivatives, updating fields, and\n",
        "   plotting the results.\n",
        "\n",
        "   Inputs and Outputs are analogous to those in BurgersEq2D, but handle two velocity fields.\n",
        "\n",
        "Usage:\n",
        "- Instantiate the desired class with the appropriate parameters,\n",
        "  and call the `burgers_driver` method with an initial condition to run the simulation.\n",
        "\"\"\"\n",
        "\n",
        "class BurgersEq2D():\n",
        "    def __init__(self,\n",
        "                 xmin=0,\n",
        "                 xmax=1,\n",
        "                 ymin=0,\n",
        "                 ymax=1,\n",
        "                 Nx=100,\n",
        "                 Ny=100,\n",
        "                 nu=0.01,\n",
        "                 dt=1e-3,\n",
        "                 tend=1.0,\n",
        "                 device=None,\n",
        "                 dtype=torch.float64,\n",
        "                 ):\n",
        "        # Initialize parameters for the 2D Burgers' equation\n",
        "        self.xmin = xmin  # Minimum x-coordinate\n",
        "        self.xmax = xmax  # Maximum x-coordinate\n",
        "        self.ymin = ymin  # Minimum y-coordinate\n",
        "        self.ymax = ymax  # Maximum y-coordinate\n",
        "        self.Nx = Nx  # Number of grid points in the x-direction\n",
        "        self.Ny = Ny  # Number of grid points in the y-direction\n",
        "        # Create linearly spaced grids in the x and y directions\n",
        "        x = torch.linspace(xmin, xmax, Nx + 1, device=device, dtype=dtype)[:-1]\n",
        "        y = torch.linspace(ymin, ymax, Ny + 1, device=device, dtype=dtype)[:-1]\n",
        "        self.x = x  # x-coordinates\n",
        "        self.y = y  # y-coordinates\n",
        "        self.dx = x[1] - x[0]  # Grid spacing in the x-direction\n",
        "        self.dy = y[1] - y[0]  # Grid spacing in the y-direction\n",
        "        self.X, self.Y = torch.meshgrid(x, y, indexing='ij')  # Create a meshgrid for plotting\n",
        "        self.nu = nu  # Viscosity coefficient\n",
        "        self.u = torch.zeros_like(self.X, device=device)  # Initialize velocity field\n",
        "        self.u0 = torch.zeros_like(self.u, device=device)  # Initial condition for velocity\n",
        "        self.dt = dt  # Time step\n",
        "        self.tend = tend  # End time\n",
        "        self.t = 0  # Current time\n",
        "        self.it = 0  # Time step counter\n",
        "        self.U = []  # List to store results\n",
        "        self.T = []  # List to store time points\n",
        "        self.device = device  # Device (CPU/GPU) to use\n",
        "\n",
        "    # Central differencing for first derivative in specified axis\n",
        "    def CD_i(self, data, axis, dx):\n",
        "        data_m2 = torch.roll(data, shifts=2, dims=axis)\n",
        "        data_m1 = torch.roll(data, shifts=1, dims=axis)\n",
        "        data_p1 = torch.roll(data, shifts=-1, dims=axis)\n",
        "        data_p2 = torch.roll(data, shifts=-2, dims=axis)\n",
        "        data_diff_i = (data_m2 - 8.0 * data_m1 + 8.0 * data_p1 - data_p2) / (12.0 * dx)\n",
        "        return data_diff_i\n",
        "\n",
        "    # Central differencing for second derivatives in two axes\n",
        "    def CD_ij(self, data, axis_i, axis_j, dx, dy):\n",
        "        data_diff_i = self.CD_i(data, axis_i, dx)\n",
        "        data_diff_ij = self.CD_i(data_diff_i, axis_j, dy)\n",
        "        return data_diff_ij\n",
        "\n",
        "    # Central differencing for second derivative in specified axis\n",
        "    def CD_ii(self, data, axis, dx):\n",
        "        data_m2 = torch.roll(data, shifts=2, dims=axis)\n",
        "        data_m1 = torch.roll(data, shifts=1, dims=axis)\n",
        "        data_p1 = torch.roll(data, shifts=-1, dims=axis)\n",
        "        data_p2 = torch.roll(data, shifts=-2, dims=axis)\n",
        "        data_diff_ii = (-data_m2 + 16.0 * data_m1 - 30.0 * data + 16.0 * data_p1 - data_p2) / (12.0 * dx ** 2)\n",
        "        return data_diff_ii\n",
        "\n",
        "    # First derivative in the x-direction\n",
        "    def Dx(self, data):\n",
        "        data_dx = self.CD_i(data=data, axis=0, dx=self.dx)\n",
        "        return data_dx\n",
        "\n",
        "    # First derivative in the y-direction\n",
        "    def Dy(self, data):\n",
        "        data_dy = self.CD_i(data=data, axis=1, dx=self.dy)\n",
        "        return data_dy\n",
        "\n",
        "    # Second derivative in the x-direction\n",
        "    def Dxx(self, data):\n",
        "        data_dxx = self.CD_ii(data, axis=0, dx=self.dx)\n",
        "        return data_dxx\n",
        "\n",
        "    # Second derivative in the y-direction\n",
        "    def Dyy(self, data):\n",
        "        data_dyy = self.CD_ii(data, axis=1, dx=self.dy)\n",
        "        return data_dyy\n",
        "\n",
        "    # Calculate the right-hand side (RHS) of the Burgers' equation\n",
        "    def burgers_calc_RHS(self, u):\n",
        "        u_xx = self.Dxx(u)  # Second derivative in x\n",
        "        u_yy = self.Dyy(u)  # Second derivative in y\n",
        "        u2 = u ** 2.0  # Square of the velocity\n",
        "        u2_x = self.Dx(u2)  # First derivative of the square in x\n",
        "        u2_y = self.Dy(u2)  # First derivative of the square in y\n",
        "        u_RHS = -0.5 * (u2_x + u2_y) + self.nu * (u_xx + u_yy)  # Calculate RHS\n",
        "        return u_RHS\n",
        "\n",
        "    # Update the velocity field based on the RHS and step fraction\n",
        "    def update_field(self, field, RHS, step_frac):\n",
        "        field_new = field + self.dt * step_frac * RHS\n",
        "        return field_new\n",
        "\n",
        "    # Merge RHS results for RK4 integration\n",
        "    def rk4_merge_RHS(self, field, RHS1, RHS2, RHS3, RHS4):\n",
        "        field_new = field + self.dt / 6.0 * (RHS1 + 2 * RHS2 + 2.0 * RHS3 + RHS4)\n",
        "        return field_new\n",
        "\n",
        "    # Runge-Kutta 4th order integration step\n",
        "    def burgers_rk4(self, u, t=0):\n",
        "        u_RHS1 = self.burgers_calc_RHS(u)  # Calculate RHS at t\n",
        "        u1 = self.update_field(u, u_RHS1, step_frac=0.5)  # Update for half-step\n",
        "\n",
        "        u_RHS2 = self.burgers_calc_RHS(u1)  # Calculate RHS at half-step\n",
        "        u2 = self.update_field(u, u_RHS2, step_frac=0.5)  # Update for half-step\n",
        "\n",
        "        u_RHS3 = self.burgers_calc_RHS(u2)  # Calculate RHS at t + dt/2\n",
        "        u3 = self.update_field(u, u_RHS3, step_frac=1.0)  # Update for full-step\n",
        "\n",
        "        u_RHS4 = self.burgers_calc_RHS(u3)  # Calculate RHS at t + dt\n",
        "\n",
        "        u_new = self.rk4_merge_RHS(u, u_RHS1, u_RHS2, u_RHS3, u_RHS4)  # Merge results\n",
        "        return u_new\n",
        "\n",
        "\n",
        "    # Main driver function to run the simulation\n",
        "    def burgers_driver(self, u0, save_interval=10):\n",
        "        self.u0 = u0[:self.Nx, :self.Ny]\n",
        "        self.u = self.u0\n",
        "        self.t = 0\n",
        "        self.it = 0\n",
        "        self.T = []\n",
        "        self.U = []\n",
        "\n",
        "        if save_interval != 0 and self.it % save_interval == 0:\n",
        "            self.U.append(self.u)\n",
        "            self.T.append(self.t)\n",
        "\n",
        "        while self.t < self.tend:\n",
        "            self.u = self.burgers_rk4(self.u, self.t)  # Only update self.u\n",
        "            self.t += self.dt  # Increment time\n",
        "\n",
        "            self.it += 1\n",
        "            if save_interval != 0 and self.it % save_interval == 0:\n",
        "                self.U.append(self.u)\n",
        "                self.T.append(self.t)\n",
        "\n",
        "        return torch.stack(self.U)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhFNz3fBLeVu"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(path, name, model, optimizer=None):\n",
        "    \"\"\"\n",
        "    Save the model and optimizer state to a checkpoint file.\n",
        "\n",
        "    This function creates a directory for checkpoints if it doesn't exist,\n",
        "    and then saves the state dictionaries of the model and optimizer (if provided)\n",
        "    to a specified file.\n",
        "\n",
        "    Args:\n",
        "        path (str): The directory path where the checkpoint will be saved.\n",
        "        name (str): The name of the checkpoint file.\n",
        "        model (nn.Module): The neural network model whose state will be saved.\n",
        "        optimizer (torch.optim.Optimizer, optional): The optimizer whose state will be saved.\n",
        "                                                     Defaults to None, which skips saving the optimizer state.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    ckpt_dir = 'checkpoints/%s/' % path  # Define the checkpoint directory based on the provided path\n",
        "    if not os.path.exists(ckpt_dir):  # Check if the directory exists\n",
        "        os.makedirs(ckpt_dir)  # Create the directory if it does not exist\n",
        "\n",
        "    try:\n",
        "        model_state_dict = model.module.state_dict()  # Get the state dict for a model wrapped in DataParallel\n",
        "    except AttributeError:\n",
        "        model_state_dict = model.state_dict()  # Get the state dict for a standard model\n",
        "\n",
        "    # Get the optimizer state if provided, otherwise set to a default value\n",
        "    if optimizer is not None:\n",
        "        optim_dict = optimizer.state_dict()  # Get the state dict for the optimizer\n",
        "    else:\n",
        "        optim_dict = 0.0  # Default value if no optimizer is provided\n",
        "\n",
        "    # Save the model and optimizer state dictionaries to the specified checkpoint file\n",
        "    torch.save({\n",
        "        'model': model_state_dict,  # Save the model state\n",
        "        'optim': optim_dict  # Save the optimizer state\n",
        "    }, ckpt_dir + name)  # Combine directory and file name for saving\n",
        "\n",
        "    print('Checkpoint is saved at %s' % (ckpt_dir + name))  # Confirm that the checkpoint has been saved"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LpLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Loss function that computes relative and/or absolute Lp losses,\n",
        "    and optionally includes Total Variation (TV) loss.\n",
        "\n",
        "    This class implements the Lp loss which can be used to measure the\n",
        "    distance between predicted and true values in various norms. Additionally,\n",
        "    it can compute the Total Variation loss to promote smoothness in the solution.\n",
        "\n",
        "    Args:\n",
        "        d (int, optional): Number of dimensions for the loss scaling. Default is 2.\n",
        "        p (float, optional): Type of Lp-norm (e.g., 1 for L1, 2 for L2). Default is 2.\n",
        "        size_average (bool, optional): If True, the losses are averaged over each loss element. Default is True.\n",
        "        reduction (str, optional): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. Default is 'mean'.\n",
        "        loss_type (str, optional): Type of loss to compute: 'rel', 'abs', 'both', or 'all'.\n",
        "                                   'all' includes relative, absolute, and TV losses. Default is 'rel'.\n",
        "        tv_weight (float, optional): Weighting factor for the Total Variation loss. Relevant only if TV loss is included. Default is 0.0.\n",
        "        input_coords (torch.Tensor, optional): Tensor containing the input coordinates corresponding to `x`. Required for TV loss computation.\n",
        "                                               Should have gradients enabled.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 d: int = 2,\n",
        "                 p: float = 2.0,\n",
        "                 size_average: bool = True,\n",
        "                 reduction: str = 'mean',\n",
        "                 loss_type: str = 'rel',\n",
        "                 tv_weight: float = 0.0,\n",
        "                 input_coords: torch.Tensor = None):\n",
        "        super(LpLoss, self).__init__()\n",
        "\n",
        "        # Validate inputs\n",
        "        if d <= 0:\n",
        "            raise ValueError(\"Number of dimensions 'd' must be positive.\")\n",
        "        if p <= 0:\n",
        "            raise ValueError(\"Lp-norm type 'p' must be positive.\")\n",
        "        if reduction not in ['none', 'mean', 'sum']:\n",
        "            raise ValueError(\"Reduction must be one of 'none', 'mean', or 'sum'.\")\n",
        "        if loss_type not in ['rel', 'abs', 'both', 'all']:\n",
        "            raise ValueError(\"Loss type must be one of 'rel', 'abs', 'both', or 'all'.\")\n",
        "        if loss_type == 'all' and tv_weight <= 0.0:\n",
        "            raise ValueError(\"For 'all' loss_type, 'tv_weight' must be positive to include TV loss.\")\n",
        "        if loss_type == 'all' and input_coords is None:\n",
        "            raise ValueError(\"For 'all' loss_type, 'input_coords' must be provided for TV loss computation.\")\n",
        "        if loss_type != 'all' and tv_weight != 0.0:\n",
        "            raise ValueError(\"tv_weight should be 0.0 unless loss_type is 'all'.\")\n",
        "\n",
        "        self.d = d\n",
        "        self.p = p\n",
        "        self.size_average = size_average\n",
        "        self.reduction = reduction\n",
        "        self.loss_type = loss_type\n",
        "        self.tv_weight = tv_weight\n",
        "        self.input_coords = input_coords  # Coordinates should have requires_grad=True\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the specified Lp loss and optionally the Total Variation loss between predicted and true values.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Predicted values. Shape: (batch_size, ..., ...)\n",
        "            y (torch.Tensor): True values. Shape: same as x.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Computed loss value.\n",
        "        \"\"\"\n",
        "        if x.shape != y.shape:\n",
        "            raise ValueError(f\"Shape mismatch: x shape {x.shape} vs y shape {y.shape}\")\n",
        "\n",
        "        # Compute Lp losses\n",
        "        loss_components = []\n",
        "\n",
        "        # Flatten the tensors except for the batch dimension\n",
        "        diff = x.view(x.size(0), -1) - y.view(y.size(0), -1)\n",
        "        diff_norm = torch.norm(diff, p=self.p, dim=1)  # Shape: (batch_size,)\n",
        "        y_norm = torch.norm(y.view(y.size(0), -1), p=self.p, dim=1)  # Shape: (batch_size,)\n",
        "\n",
        "        # Compute absolute loss\n",
        "        h = 1.0 / (x.size(1) - 1.0)  # Assuming uniform mesh along the second dimension\n",
        "        abs_loss = (h ** (self.d / self.p)) * diff_norm\n",
        "        loss_components.append(abs_loss)\n",
        "\n",
        "        # Compute relative loss with epsilon to prevent division by zero\n",
        "        epsilon = 1e-12\n",
        "        rel_loss = diff_norm / (y_norm + epsilon)\n",
        "        loss_components.append(rel_loss)\n",
        "\n",
        "        # Initialize total loss\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # Handle different loss types\n",
        "        if self.loss_type == 'rel':\n",
        "            total_loss = rel_loss\n",
        "        elif self.loss_type == 'abs':\n",
        "            total_loss = abs_loss\n",
        "        elif self.loss_type == 'both':\n",
        "            total_loss = rel_loss + abs_loss\n",
        "        elif self.loss_type == 'all':\n",
        "            # Compute TV loss\n",
        "            tv_loss = self.compute_total_variation(x)\n",
        "            total_loss = rel_loss + abs_loss + self.tv_weight * tv_loss\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n",
        "\n",
        "        # Apply reduction\n",
        "        if self.reduction == 'mean':\n",
        "            return total_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return total_loss.sum()\n",
        "        else:  # 'none'\n",
        "            return total_loss\n",
        "\n",
        "    def compute_total_variation(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the Total Variation (TV) loss for the predicted tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Predicted values. Shape: (batch_size, channels, ...)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: TV loss value per batch.\n",
        "        \"\"\"\n",
        "        if self.input_coords is None:\n",
        "            raise ValueError(\"Input coordinates must be provided for TV loss computation.\")\n",
        "\n",
        "        # Ensure input_coords requires gradients\n",
        "        if not self.input_coords.requires_grad:\n",
        "            raise ValueError(\"input_coords must have requires_grad=True for TV loss computation.\")\n",
        "\n",
        "        # Compute gradients with respect to input coordinates\n",
        "        grads = torch.autograd.grad(\n",
        "            outputs=x,\n",
        "            inputs=self.input_coords,\n",
        "            grad_outputs=torch.ones_like(x),\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "            only_inputs=True\n",
        "        )[0]  # Shape: same as input_coords\n",
        "\n",
        "        # Compute the L1 norm of gradients (Total Variation)\n",
        "        # Assuming input_coords has spatial dimensions; sum over spatial dims\n",
        "        # For example, input_coords shape: (batch_size, num_dims, H, W) for 2D\n",
        "        tv_loss = torch.sum(torch.abs(grads), dim=1)  # Sum over the spatial dimensions\n",
        "\n",
        "        # Depending on the desired reduction, return appropriate value\n",
        "        # Here, we return the mean TV loss per batch\n",
        "        return tv_loss.mean()"
      ],
      "metadata": {
        "id": "pqC2de6Fjs_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_burgers2d(\n",
        "    model,\n",
        "    dataset,\n",
        "    train_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    nu=0.01,                   # Viscosity coefficient for the Burgers' equation\n",
        "    rank=0,\n",
        "    log=False,\n",
        "    use_tqdm=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the PINO model for the 2D Burgers' equation.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train.\n",
        "        dataset: Dataset containing the training data.\n",
        "        train_loader: DataLoader for batching the training data.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
        "        scheduler: Learning rate scheduler for adjusting the learning rate.\n",
        "        nu (float): Viscosity coefficient for the Burgers' equation.\n",
        "        rank (int): Rank of the process (for distributed training).\n",
        "        log (bool): Whether to log metrics.\n",
        "        use_tqdm (bool): Whether to use tqdm for progress visualization.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Extract weights for different loss components\n",
        "    data_weight = 10.0\n",
        "    f_weight = 1.0\n",
        "    ic_weight = 10.0\n",
        "    ckpt_freq = 25\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "    myloss = LpLoss(size_average=True)  # Loss function for data\n",
        "    S, T = dataset.S, dataset.T  # Extract spatial and temporal information from the dataset\n",
        "\n",
        "    # Progress bar setup\n",
        "    pbar = range(150)  # config['train']['epochs']\n",
        "    if use_tqdm:\n",
        "        pbar = tqdm(pbar, dynamic_ncols=True, smoothing=0.1)\n",
        "\n",
        "    # Initialize max memory tracking\n",
        "    max_memory_usage = 0\n",
        "\n",
        "    for e in pbar:\n",
        "        train_loss = 0.0  # Initialize cumulative training loss\n",
        "\n",
        "        for x, y in train_loader:  # Iterate over batches of data\n",
        "            x, y = x.to(rank), y.to(rank)  # Move data to the appropriate device\n",
        "            out = model(x).reshape(y.shape)  # Model output reshaped to match target shape\n",
        "\n",
        "            # Calculate data loss\n",
        "            data_loss = myloss(out, y)\n",
        "\n",
        "            # Compute the initial condition and PDE loss\n",
        "            loss_ic, loss_f = PINO_loss_burgers2D(out, x[..., 0, -1], nu=nu)\n",
        "            total_loss = loss_ic * ic_weight + loss_f * f_weight + data_loss * data_weight  # Total loss\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            total_loss.backward()  # Backpropagate to compute gradients\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            # Accumulate losses for reporting\n",
        "            train_loss += total_loss.item()\n",
        "\n",
        "            # Track memory usage during training\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()  # Ensure all computations are done\n",
        "                max_memory_usage = max(max_memory_usage, torch.cuda.max_memory_allocated(rank))\n",
        "            else:\n",
        "                process = psutil.Process()\n",
        "                max_memory_usage = max(max_memory_usage, process.memory_info().rss)\n",
        "\n",
        "        scheduler.step()  # Step the learning rate scheduler\n",
        "\n",
        "        # Calculate average losses for reporting\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        if use_tqdm:\n",
        "            pbar.set_description(f'Epoch {e}, train loss: {train_loss:.5f}')\n",
        "\n",
        "        # Save checkpoints at specified intervals\n",
        "        if e % ckpt_freq == 0:\n",
        "            save_checkpoint(\n",
        "                'Burgers2D',\n",
        "                'Burgers2D-0001.pt'.replace('.pt', f'_{e}.pt'),\n",
        "                model, optimizer\n",
        "            )\n",
        "\n",
        "    # Save the final model checkpoint\n",
        "    save_checkpoint(\n",
        "        'Burgers2D',\n",
        "        'Burgers2D-0001.pt',\n",
        "        model, optimizer\n",
        "    )\n",
        "\n",
        "    # Print the maximum memory usage at the end\n",
        "    if torch.cuda.is_available():\n",
        "        print(f'Maximum GPU memory allocated during training: {max_memory_usage / (1024 ** 2):.2f} MB')\n",
        "    else:\n",
        "        print(f'Maximum CPU memory used during training: {max_memory_usage / (1024 ** 2):.2f} MB')\n",
        "\n",
        "    print('Training complete!')\n"
      ],
      "metadata": {
        "id": "TLxAO8fxkTvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tsGuaXlIwrS"
      },
      "outputs": [],
      "source": [
        "def eval_burgers2D(model,\n",
        "                   dataloader,\n",
        "                   device,\n",
        "                   nu=0.01,\n",
        "                   use_tqdm=True):\n",
        "    \"\"\"\n",
        "    Evaluate the PINO model for the 2D Burgers' equation.\n",
        "\n",
        "    This function evaluates the model's performance on a given dataset, calculating\n",
        "    the average L2 error and the error associated with the Burgers' equation. It\n",
        "    operates in evaluation mode to disable gradient calculations.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained neural network model to evaluate.\n",
        "        dataloader: DataLoader containing the test data.\n",
        "        config (dict): Configuration dictionary for the evaluation settings.\n",
        "        device (torch.device): Device to perform the computation (CPU or GPU).\n",
        "        nu (float): Viscosity coefficient for the Burgers' equation.\n",
        "        use_tqdm (bool): Whether to use tqdm for progress visualization.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    myloss = LpLoss(size_average=True)  # Define the loss function for data evaluation\n",
        "\n",
        "    # Set up progress bar\n",
        "    pbar = tqdm(dataloader, dynamic_ncols=True, smoothing=0.05) if use_tqdm else dataloader\n",
        "\n",
        "    test_err = []  # List to store data loss for evaluation\n",
        "    f_err = []  # List to store equation loss for evaluation\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for x, y in pbar:\n",
        "            x, y = x.to(device), y.to(device)  # Move data to the specified device\n",
        "            out = model(x).reshape(y.shape)  # Get model output and reshape to match target shape\n",
        "\n",
        "            # Calculate data loss\n",
        "            data_loss = myloss(out, y)\n",
        "\n",
        "            # Compute the initial condition and PDE loss\n",
        "            loss_ic, f_loss = PINO_loss_burgers2D(out, x[..., 0, -1], nu=nu)\n",
        "            test_err.append(data_loss.item())  # Store data loss\n",
        "            f_err.append(f_loss.item())  # Store PDE loss\n",
        "\n",
        "    # Calculate mean and standard deviation of errors\n",
        "    mean_f_err = np.mean(f_err)  # Mean of equation errors\n",
        "    std_f_err = np.std(f_err, ddof=1) / np.sqrt(len(f_err))  # Standard error of equation errors\n",
        "\n",
        "    mean_err = np.mean(test_err)  # Mean of data errors\n",
        "    std_err = np.std(test_err, ddof=1) / np.sqrt(len(test_err))  # Standard error of data errors\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(f'==Averaged relative L2 error mean: {mean_err:.5f}, std error: {std_err:.5f}==\\n'\n",
        "          f'==Averaged equation error mean: {mean_f_err:.5f}, std error: {std_f_err:.5f}==')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oME-MB5KJopV"
      },
      "outputs": [],
      "source": [
        "class DataLoader2D(object):\n",
        "    \"\"\"\n",
        "    A custom DataLoader for handling 2D data for training and evaluation of models\n",
        "    related to the Burgers' equation or similar tasks.\n",
        "\n",
        "    This class allows for subsampling of the input data and provides functionality\n",
        "    to create DataLoader instances for batching the data.\n",
        "\n",
        "    Attributes:\n",
        "        sub (int): Factor for subsampling in the spatial dimensions.\n",
        "        sub_t (int): Factor for subsampling in the temporal dimension.\n",
        "        S (int): Effective size of the spatial grid after subsampling.\n",
        "        T (int): Effective size of the temporal grid after subsampling.\n",
        "        data (torch.Tensor): Processed tensor containing the input data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, nx=128, nt=100, sub=1, sub_t=1):\n",
        "        \"\"\"\n",
        "        Initialize the DataLoader2D instance.\n",
        "\n",
        "        Args:\n",
        "            data (torch.Tensor): Input tensor of shape (batch_size, nt, nx, nx).\n",
        "            nx (int): Size of the spatial dimension.\n",
        "            nt (int): Size of the temporal dimension.\n",
        "            sub (int): Subsampling factor for the spatial dimensions.\n",
        "            sub_t (int): Subsampling factor for the temporal dimension.\n",
        "        \"\"\"\n",
        "        self.sub = sub  # Store the spatial subsampling factor\n",
        "        self.sub_t = sub_t  # Store the temporal subsampling factor\n",
        "        s = nx  # Initialize size variable with the spatial dimension size\n",
        "\n",
        "        # Ensure nx is even for consistent processing\n",
        "        if (s % 2) == 1:\n",
        "            s = s - 1  # Reduce size by 1 if odd\n",
        "\n",
        "        # Calculate effective sizes after subsampling\n",
        "        self.S = s // sub  # Effective spatial dimension after subsampling\n",
        "        self.T = nt // sub_t  # Effective temporal dimension after subsampling\n",
        "        self.T += 1  # Increment T to include the initial condition\n",
        "\n",
        "        # Subsample the input data tensor\n",
        "        data = data[:, 0:self.T:sub_t, 0:self.S:sub, 0:self.S:sub]\n",
        "        self.data = data.permute(0, 2, 3, 1)  # Rearrange dimensions to (batch_size, S, S, T)\n",
        "\n",
        "    def make_loader(self, n_sample, batch_size, start=0, train=True):\n",
        "        \"\"\"\n",
        "        Create a DataLoader for training or evaluation.\n",
        "\n",
        "        Args:\n",
        "            n_sample (int): Number of samples to include in the DataLoader.\n",
        "            batch_size (int): Number of samples per batch.\n",
        "            start (int): Starting index for the samples in the dataset.\n",
        "            train (bool): Whether to shuffle the data for training or not.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: A DataLoader instance for the specified samples.\n",
        "        \"\"\"\n",
        "        # Extract and reshape the data for the DataLoader\n",
        "        a_data = self.data[start:start + n_sample, :, :, 0].reshape(n_sample, self.S, self.S)  # Extract auxiliary data\n",
        "        u_data = self.data[start:start + n_sample].reshape(n_sample, self.S, self.S, self.T)  # Extract target data\n",
        "\n",
        "        # Generate the grid for 3D data\n",
        "        gridx, gridy, gridt = get_grid3d(self.S, self.T)\n",
        "\n",
        "        # Expand auxiliary data dimensions and repeat for the time dimension\n",
        "        a_data = a_data.reshape(n_sample, self.S, self.S, 1, 1).repeat([1, 1, 1, self.T, 1])\n",
        "\n",
        "        # Concatenate grid information with auxiliary data\n",
        "        a_data = torch.cat((gridx.repeat([n_sample, 1, 1, 1, 1]),\n",
        "                            gridy.repeat([n_sample, 1, 1, 1, 1]),\n",
        "                            gridt.repeat([n_sample, 1, 1, 1, 1]),\n",
        "                            a_data), dim=-1)\n",
        "\n",
        "        # Create a TensorDataset from the auxiliary and target data\n",
        "        dataset = torch.utils.data.TensorDataset(a_data, u_data)\n",
        "\n",
        "        # Create a DataLoader with shuffling for training or without shuffling for evaluation\n",
        "        if train:\n",
        "            loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        else:\n",
        "            loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        return loader  # Return the DataLoader instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc65z5vxmuVH"
      },
      "outputs": [],
      "source": [
        "def get_grid3d(S, T, time_scale=1.0, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generate 3D grids for spatial and temporal coordinates.\n",
        "\n",
        "    This function creates 3D grids representing the spatial dimensions (gridx, gridy)\n",
        "    and the temporal dimension (gridt) used in simulations or numerical methods related\n",
        "    to 2D problems. The grids are constructed based on the specified sizes for the spatial\n",
        "    and temporal dimensions and can be placed on a specified device (CPU or GPU).\n",
        "\n",
        "    Args:\n",
        "        S (int): Size of the spatial grid (number of spatial points).\n",
        "        T (int): Size of the temporal grid (number of time points).\n",
        "        time_scale (float, optional): Scale factor for the time dimension (default: 1.0).\n",
        "        device (str, optional): Device to store the generated tensors (default: 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "            - gridx (torch.Tensor): 3D tensor representing the x-coordinates.\n",
        "            - gridy (torch.Tensor): 3D tensor representing the y-coordinates.\n",
        "            - gridt (torch.Tensor): 3D tensor representing the time-coordinates.\n",
        "    \"\"\"\n",
        "    # Create a 1D grid for the x-coordinates, excluding the last point\n",
        "    gridx = torch.tensor(np.linspace(0, 1, S + 1)[:-1], dtype=torch.float, device=device)\n",
        "    # Reshape and repeat to create a 3D grid for x-coordinates\n",
        "    gridx = gridx.reshape(1, S, 1, 1, 1).repeat([1, 1, S, T, 1])\n",
        "\n",
        "    # Create a 1D grid for the y-coordinates, excluding the last point\n",
        "    gridy = torch.tensor(np.linspace(0, 1, S + 1)[:-1], dtype=torch.float, device=device)\n",
        "    # Reshape and repeat to create a 3D grid for y-coordinates\n",
        "    gridy = gridy.reshape(1, 1, S, 1, 1).repeat([1, S, 1, T, 1])\n",
        "\n",
        "    # Create a 1D grid for the time-coordinates scaled by time_scale\n",
        "    gridt = torch.tensor(np.linspace(0, 1 * time_scale, T), dtype=torch.float, device=device)\n",
        "    # Reshape and repeat to create a 3D grid for time-coordinates\n",
        "    gridt = gridt.reshape(1, 1, 1, T, 1).repeat([1, S, S, 1, 1])\n",
        "\n",
        "    return gridx, gridy, gridt  # Return the generated grids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bge_qI7rJDzU"
      },
      "outputs": [],
      "source": [
        "dim = 2\n",
        "N = 128\n",
        "Nx = N\n",
        "Ny = N\n",
        "Nt = 100 + 1\n",
        "Ntest = 5\n",
        "l = 0.1\n",
        "L = 1.0\n",
        "sigma = 0.2 #2.0\n",
        "nu = 0.01\n",
        "Nu = None #2.0\n",
        "Nsamples = 50\n",
        "jitter = 1e-12\n",
        "dt=1.0e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRFC9zj4JI57"
      },
      "outputs": [],
      "source": [
        "with open('drive/MyDrive/U02.pkl', 'rb') as f:\n",
        "  U0 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUhzNXl0PW3w"
      },
      "outputs": [],
      "source": [
        "burgers_eq = BurgersEq2D(Nx=Nx, Ny=Ny, dt=dt, nu=nu, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5awGEPGdaEm8"
      },
      "outputs": [],
      "source": [
        "U = vmap(burgers_eq.burgers_driver, in_dims=(0, None))(U0, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoWa5nxLZLTc"
      },
      "outputs": [],
      "source": [
        "a = U0.cpu().float()\n",
        "u = U.cpu().float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3m1AENfJVgc"
      },
      "outputs": [],
      "source": [
        "dataset = DataLoader2D(\n",
        "    u,\n",
        "    nx=128,\n",
        "    nt=100,\n",
        "    sub=1,\n",
        "    sub_t=1\n",
        ")\n",
        "\n",
        "train_loader = dataset.make_loader(\n",
        "    n_sample=45,\n",
        "    batch_size=1,\n",
        "    start=0,\n",
        "    train=True\n",
        ")\n",
        "\n",
        "test_loader = dataset.make_loader(\n",
        "    n_sample=5,\n",
        "    batch_size=1,\n",
        "    start=45,\n",
        "    train=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGBX1WwGKcqm"
      },
      "outputs": [],
      "source": [
        "log = False\n",
        "\n",
        "# Create the model using direct values from the configuration\n",
        "model = FNN3d(\n",
        "    modes1=[8, 8, 8, 8],\n",
        "    modes2=[8, 8, 8, 8],\n",
        "    modes3=[8, 8, 8, 8],\n",
        "    fc_dim=500,\n",
        "    layers=[128, 128, 128, 128],\n",
        "    activation='prelu'\n",
        ").to(device)\n",
        "\n",
        "# Initialize the optimizer with direct values\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=0.5e-3,\n",
        "    weight_decay=0.5e-3\n",
        ")\n",
        "\n",
        "# Initialize the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones=[25, 50, 75, 100],\n",
        "    gamma=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zc0KZJwK2KI",
        "outputId": "9fabdc0b-1023-4368-9c84-fdb79071bd67"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0, train loss: 10.58889:   1%|          | 1/150 [00:24<1:01:47, 24.89s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_0.pt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25, train loss: 2.14360:  17%|        | 26/150 [09:55<47:15, 22.87s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_25.pt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50, train loss: 1.79612:  34%|      | 51/150 [19:26<37:44, 22.87s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_50.pt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 75, train loss: 1.35685:  51%|     | 76/150 [28:57<28:12, 22.87s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_75.pt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 100, train loss: 1.10235:  67%|   | 101/150 [38:28<18:41, 22.88s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_100.pt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 125, train loss: 0.99471:  84%| | 126/150 [48:00<09:09, 22.91s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_125.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 149, train loss: 0.90784: 100%|| 150/150 [57:08<00:00, 22.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001.pt\n",
            "Maximum GPU memory allocated during training: 18677.72 MB\n",
            "Training complete!\n",
            "0:57:09.014705\n"
          ]
        }
      ],
      "source": [
        "start = timer()\n",
        "\n",
        "# Call the train_burgers2d function with direct values\n",
        "train_burgers2d(\n",
        "    model,\n",
        "    dataset,\n",
        "    train_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    nu=0.01,\n",
        "    rank=0,\n",
        "    log=log,\n",
        "    use_tqdm=True\n",
        ")\n",
        "\n",
        "end = timer()\n",
        "print(timedelta(seconds=end - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aUgIMWggNJUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8272e0d-5f5b-44b4-e1db-906c3f538d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:01<00:00,  4.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==Averaged relative L2 error mean: 0.19749, std error: 0.00891==\n",
            "==Averaged equation error mean: 0.00600, std error: 0.00064==\n",
            "0:00:01.037934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "start = timer()\n",
        "\n",
        "# Call the eval_burgers2D function with direct values\n",
        "eval_burgers2D(\n",
        "    model,\n",
        "    test_loader,\n",
        "    device,\n",
        "    nu=0.01,\n",
        "    use_tqdm=True\n",
        ")\n",
        "\n",
        "end = timer()\n",
        "print(timedelta(seconds=end - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By6SZaeZ73FS"
      },
      "outputs": [],
      "source": [
        "class SpectralConv3d(nn.Module):\n",
        "    \"\"\"\n",
        "    A spectral convolution layer that applies the Real Fast Fourier Transform (FFT)\n",
        "    to perform convolution in the frequency domain for 3D tensors.\n",
        "\n",
        "    This layer takes 3D input tensors and applies a convolution operation using a\n",
        "    specified number of FFT modes.\n",
        "\n",
        "    Attributes:\n",
        "        in_channels (int): Number of input channels.\n",
        "        out_channels (int): Number of output channels.\n",
        "        modes1 (int): Number of FFT modes to use in the first dimension.\n",
        "        modes2 (int): Number of FFT modes to use in the second dimension.\n",
        "        modes3 (int): Number of FFT modes to use in the third dimension.\n",
        "        scale (float): Scaling factor for the weights initialization.\n",
        "        weights1 (nn.Parameter): Learnable complex weights for the convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, modes1, modes2, modes3):\n",
        "        \"\"\"\n",
        "        Initialize the SpectralConv3d layer.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels.\n",
        "            out_channels (int): Number of output channels.\n",
        "            modes1 (int): Number of FFT modes in the first dimension.\n",
        "            modes2 (int): Number of FFT modes in the second dimension.\n",
        "            modes3 (int): Number of FFT modes in the third dimension.\n",
        "        \"\"\"\n",
        "        super(SpectralConv3d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.modes1 = modes1\n",
        "        self.modes2 = modes2\n",
        "        self.modes3 = modes3\n",
        "\n",
        "        self.scale = (1 / (in_channels * out_channels))\n",
        "        # Initialize real and imaginary parts separately\n",
        "        self.weights1_real = nn.Parameter(\n",
        "            self.scale * torch.randn(in_channels, out_channels, modes1, modes2, modes3)\n",
        "        )\n",
        "        self.weights1_imag = nn.Parameter(\n",
        "            self.scale * torch.randn(in_channels, out_channels, modes1, modes2, modes3)\n",
        "        )\n",
        "\n",
        "    def fft_forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform the forward real FFT on the input tensor.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, channels, depth, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: FFT-transformed tensor with complex dtype.\n",
        "        \"\"\"\n",
        "        return torch.fft.rfftn(x, dim=(-3, -2, -1))\n",
        "\n",
        "    def fft_inverse(self, x: torch.Tensor, original_size: tuple) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform the inverse real FFT to return to the physical space.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): FFT-transformed tensor with complex dtype.\n",
        "            original_size (tuple): The original size (depth, height, width) of the input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Inverse FFT-transformed tensor of shape (batch_size, channels, depth, height, width).\n",
        "        \"\"\"\n",
        "        return torch.fft.irfftn(x, s=original_size, dim=(-3, -2, -1))\n",
        "\n",
        "    def compl_mul3d(self, input_fft: torch.Tensor, weights_fft: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform complex multiplication between input FFT and weights FFT.\n",
        "\n",
        "        Args:\n",
        "            input_fft (torch.Tensor): FFT of input with shape (batch_size, in_channels, modes1, modes2, modes3).\n",
        "            weights_fft (torch.Tensor): FFT weights with shape (in_channels, out_channels, modes1, modes2, modes3).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Result of complex multiplication with shape (batch_size, out_channels, modes1, modes2, modes3).\n",
        "        \"\"\"\n",
        "        return torch.einsum(\"bixyz,ioxyz->boxyz\", input_fft, weights_fft)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the spectral convolution layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_channels, depth, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, out_channels, depth, height, width).\n",
        "        \"\"\"\n",
        "        batchsize = x.shape[0]\n",
        "        depth, height, width = x.shape[-3], x.shape[-2], x.shape[-1]\n",
        "\n",
        "        # Compute FFT of the input\n",
        "        x_fft = self.fft_forward(x)  # Shape: (batch_size, in_channels, modes1, modes2, modes3)\n",
        "\n",
        "        # Initialize FFT of the output with complex dtype\n",
        "        out_fft = torch.zeros(\n",
        "            batchsize,\n",
        "            self.out_channels,\n",
        "            self.modes1,\n",
        "            self.modes2,\n",
        "            self.modes3,\n",
        "            dtype=torch.cfloat,\n",
        "            device=x.device,\n",
        "        )\n",
        "\n",
        "        # Combine real and imaginary parts to form complex weights\n",
        "        weights_fft = torch.complex(self.weights1_real, self.weights1_imag)  # Shape: (in_channels, out_channels, modes1, modes2, modes3)\n",
        "\n",
        "        # Perform complex multiplication on the relevant FFT modes\n",
        "        out_fft[:, :, :self.modes1, :self.modes2, :self.modes3] = self.compl_mul3d(\n",
        "            x_fft[:, :, :self.modes1, :self.modes2, :self.modes3],\n",
        "            weights_fft\n",
        "        )\n",
        "\n",
        "        # Inverse FFT to return to physical space\n",
        "        x = self.fft_inverse(out_fft, original_size=(depth, height, width))  # Shape: (batch_size, out_channels, depth, height, width)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNsriWltgXQc"
      },
      "outputs": [],
      "source": [
        "log = False\n",
        "\n",
        "# Create the model using direct values from the configuration\n",
        "model = FNN3d(\n",
        "    modes1=[8, 8, 8, 8],\n",
        "    modes2=[8, 8, 8, 8],\n",
        "    modes3=[8, 8, 8, 8],\n",
        "    fc_dim=256,\n",
        "    layers=[128, 128, 128, 128],\n",
        "    activation='elu'\n",
        ").to(device)\n",
        "\n",
        "# Initialize the optimizer with direct values\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "# Initialize the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones=[25, 50, 75, 100],\n",
        "    gamma=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-Y74SWOgRGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47168ce9-22ae-4451-ed8f-0e0a2aa16f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0, train loss: 13.21085:   1%|          | 1/150 [00:16<41:33, 16.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_0.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25, train loss: 1.28888:  17%|        | 26/150 [06:41<32:00, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_25.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50, train loss: 0.49538:  34%|      | 51/150 [13:06<25:32, 15.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_50.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 75, train loss: 0.28754:  51%|     | 76/150 [19:31<19:05, 15.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_75.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 100, train loss: 0.20959:  67%|   | 101/150 [25:56<12:40, 15.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_100.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 125, train loss: 0.18869:  84%| | 126/150 [32:21<06:11, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint is saved at checkpoints/Burgers2D/Burgers2D-0001_125.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 128, train loss: 0.18308:  86%| | 129/150 [33:07<05:24, 15.45s/it]"
          ]
        }
      ],
      "source": [
        "start = timer()\n",
        "\n",
        "# Call the train_burgers2d function with direct values\n",
        "train_burgers2d(\n",
        "    model,\n",
        "    dataset,\n",
        "    train_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    nu=0.01,\n",
        "    rank=0,\n",
        "    log=log,\n",
        "    use_tqdm=True\n",
        ")\n",
        "\n",
        "end = timer()\n",
        "print(timedelta(seconds=end - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJPcid74uqG7"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "\n",
        "# Call the eval_burgers2D function with direct values\n",
        "eval_burgers2D(\n",
        "    model,\n",
        "    test_loader,\n",
        "    device,\n",
        "    nu=0.01,\n",
        "    use_tqdm=True\n",
        ")\n",
        "\n",
        "end = timer()\n",
        "print(timedelta(seconds=end - start))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMqs45KpE0I1ZFD9BvEo4yv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}